### Tâche 1 : Compréhension de l’algorithme GBoost

(1) Expliquez le principe de fonctionnement de l’algorithme de boosting de gradient (GBoost).  
(2) Soulignez ses forces et avantages comparatifs.

## GBoost : introduction

L'algorithme GBoost est une méthode d'apprentissage automatique utilisée pour améliorer la performance des modèles de prédiction, notamment les arbres de décision. Il appartient à la famille des algorithmes d’ensemble et repose sur l’idée d’entraîner plusieurs modèles faibles de manière séquentielle afin de réduire progressivement l’erreur. Contrairement à une forêt aléatoire, qui entraîne des arbres en parallèle et agrège leurs résultats, GBoost ajuste chaque nouvel arbre en fonction des erreurs des arbres précédents pour corriger leurs prédictions.

Les arbres de décision seuls présentent plusieurs limitations. Ils sont très sensibles aux données d’apprentissage et peuvent soit sous-ajuster, en ne capturant pas suffisamment les tendances, soit sur-ajuster, en s’adaptant trop aux spécificités des données et en perdant leur capacité de généralisation. De plus, leur performance est fortement influencée par la structure des données et par la qualité de l’étiquetage. Dans un contexte où l’on dispose d’un volume suffisant de données bien structurées et où l’objectif est d’atteindre une haute précision, GBoost permet d’améliorer un arbre de décision en l’intégrant dans un processus itératif de correction des erreurs.

## Principe de fonctionnement

GBoost construit une série d’arbres de décision de manière séquentielle, où chaque nouvel arbre est entraîné pour minimiser l’erreur des prédictions accumulées. L’idée centrale est que chaque nouvel arbre vient **corriger les erreurs du modèle précédent** en se basant sur la direction qui permet de réduire la fonction de perte.

À chaque itération, l’algorithme évalue **dans quelle direction** ajuster les prédictions en calculant la dérivée de la fonction de perte par rapport aux prédictions actuelles. Ce gradient, qui représente l’erreur résiduelle, est défini comme :

$$ r\_{i}^{(t)} = - \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} $$

où $ r\_{i}^{(t)} $ est la correction que doit apprendre le nouvel arbre. Au lieu d’apprendre directement à prédire $ y $, chaque nouvel arbre est entraîné sur ces résidus afin de réduire progressivement l’écart entre les prédictions du modèle et les vraies valeurs.

Une fois cet arbre ajusté, les prédictions du modèle sont mises à jour en ajoutant la sortie de cet arbre pondérée par un facteur d’apprentissage :

$$ F*{t}(x) = F*{t-1}(x) + \alpha h_t(x) $$

où $ h_t(x) $ est le nouvel arbre entraîné et $ \alpha $ un coefficient qui contrôle son influence.

Pour illustrer le fonctionnement de GBoost, prenons un exemple simple de prédiction numérique. Supposons que nous voulons estimer une valeur $ y $ en fonction d’une variable $ x $. Nous avons trois observations : $ (x_1 = 1, y_1 = 5) $, $ (x_2 = 2, y_2 = 7) $, $ (x_3 = 3, y_3 = 10) $. L’algorithme commence avec une estimation initiale naïve, souvent la moyenne des valeurs cibles, ici $ F_0(x) = 7.33 $. Il calcule ensuite l’erreur résiduelle pour chaque point, soit $ r_1 = 5 - 7.33 = -2.33 $, $ r_2 = 7 - 7.33 = -0.33 $, et $ r_3 = 10 - 7.33 = 2.67 $. Un premier arbre est entraîné pour prédire ces erreurs résiduelles. Cet arbre ne produit pas nécessairement des valeurs exactes, mais génère une approximation, par exemple $ h_1(x) = -2 $ pour $ x_1 $, $ 0 $ pour $ x_2 $ et $ 2 $ pour $ x_3 $. L’algorithme met alors à jour les prédictions en ajoutant ces corrections pondérées par un taux d’apprentissage $ \alpha = 0.5 $, donnant $ F_1(x_1) = 7.33 + 0.5(-2) = 6.33 $, $ F_1(x_2) = 7.33 + 0.5(0) = 7.33 $, et $ F_1(x_3) = 7.33 + 0.5(2) = 8.33 $. De nouvelles erreurs sont calculées : $ r_1 = 5 - 6.33 = -1.33 $, $ r_2 = 7 - 7.33 = -0.33 $, et $ r_3 = 10 - 8.33 = 1.67 $. Un second arbre est alors entraîné sur ces nouveaux résidus, puis ses prédictions sont intégrées au modèle de la même manière.

L’algorithme répète ce processus sur plusieurs itérations, construisant progressivement un ensemble d’arbres capables d’affiner les prédictions. Un critère d’arrêt est utilisé pour éviter un apprentissage excessif. Celui-ci peut être basé sur une validation croisée ou défini par une condition sur la diminution de la fonction de perte, qui dépend du type de tâche.

Pour la **régression**, on utilise généralement l’erreur quadratique moyenne (MSE), qui mesure la différence entre les valeurs réelles et prédites sous forme d’une moyenne des carrés des écarts. **L’erreur quadratique moyenne (MSE)**, définie par :

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - F(x_i))^2
$$

où $ y_i $ est la valeur réelle et $ F(x_i) $ est la prédiction du modèle. La **descente de gradient** consiste à ajuster les prédictions en suivant la dérivée de cette fonction de perte par rapport aux prédictions actuelles, ce qui donne l’erreur résiduelle :

$$
r_i = y_i - F(x_i)
$$

C’est ainsi que l’on obtient, dans notre exemple précédent, $ r_1 = 5 - 7.33 = -2.33 $, $ r_2 = 7 - 7.33 = -0.33 $, et $ r_3 = 10 - 7.33 = 2.67 $.

Pour la classification, une fonction de perte courante est la log-loss, qui quantifie l’écart entre les probabilités prédites et les vraies classes sous forme d’une perte logarithmique. **Log-loss** (aussi appelée perte logarithmique), qui est définie par :

$$
\text{Log-loss} = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
$$

où $ p_i $ est la probabilité prédite pour la classe $ y_i = 1 $. Dans ce cas, le gradient est calculé en fonction de la différence entre la probabilité prédite et la vraie classe.

Le taux d’apprentissage $ \alpha $ est un coefficient qui contrôle l’impact de chaque nouvel arbre dans la mise à jour des prédictions, permettant d’ajuster progressivement le modèle : un $ \alpha $ élevé accélère l’apprentissage mais peut provoquer un surajustement, tandis qu’un $ \alpha $ faible assure une correction plus lente et stable, favorisant une meilleure généralisation.

L’algorithme peut s’arrêter lorsque la réduction de la perte devient négligeable ou lorsque la perte commence à augmenter, indiquant un surajustement au jeu de données d’entraînement.

Lorsqu’une nouvelle donnée doit être classifiée, elle passe dans chaque arbre, et la classe finale est déterminée en combinant les sorties pondérées de tous les arbres.

## Forces et avantages

L’algorithme GBoost offre plusieurs avantages. Tout d’abord, il permet d’obtenir des performances élevées en ajustant finement les erreurs du modèle à chaque itération, ce qui améliore la précision par rapport à un simple arbre de décision ou à une forêt aléatoire. Ensuite, il est flexible, car il peut être appliqué à divers types de problèmes en ajustant la fonction de perte et les hyperparamètres tels que la profondeur des arbres, le taux d’apprentissage et le nombre d’arbres.

Toutefois, cette approche présente des inconvénients. L’entraînement des arbres étant séquentiel, GBoost est plus lent que des méthodes parallélisables comme la forêt aléatoire. Bien que la construction d’un arbre puisse être parallélisée dans certaines implémentations, l’apprentissage global reste dépendant des itérations précédentes, ce qui limite les gains en performance.

GBoost peut également être sujet au surajustement si le nombre d’arbres est trop élevé ou si le modèle apprend trop fidèlement les données d’entraînement. Pour éviter cela, il est recommandé d’utiliser un taux d’apprentissage faible, associé à un nombre plus important d’arbres, ce qui permet une correction plus progressive des erreurs et une meilleure généralisation. Des variantes comme XGBoost introduisent des techniques de régularisation supplémentaires pour améliorer la robustesse et la rapidité d’entraînement.

En comparaison avec d’autres méthodes d’ensemble, GBoost est particulièrement efficace lorsque les données d’entrée sont bien structurées et que la précision est un critère important. Son interprétabilité est également un avantage, car il est possible d’examiner l’importance des différentes variables et la contribution de chaque arbre aux prédictions finales.

En résumé, GBoost est un algorithme puissant qui améliore la précision des modèles prédictifs en corrigeant itérativement leurs erreurs, mais qui nécessite un réglage minutieux pour éviter le surajustement et limiter le temps d’entraînement.
